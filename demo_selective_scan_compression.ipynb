{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the necessary packages and the selective-scan compression module (i.e., MambaCompressor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/bum/mmiemon/miniconda3/envs/bimba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from llava.model.multimodal_resampler.mamba_ssm.modules.mamba_compressor import MambaCompressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are given a video of 64 frames, and each frame has (24x24) tokens. We want to compress the temporal dimension 4 times (i.e., 16 frames) and height and width by 2 times. So, we want our output compressed tokens to be of shape (16, 12, 12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "input_shape = (64, 24, 24)\n",
    "hidden_size = 1024\n",
    "target_shape = (16, 12, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's initialize the MambaCompressor model. We initialize the output projection of the last mamba layer of our model from zero. This is very important for stable optimization if you want to start training our model from a pretrained checkpoint so that it does not break any pretrained weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.norm.weight 1024.0\n",
      "layers.0.mixer.A_log 62815.97265625\n",
      "layers.0.mixer.D 2048.0\n",
      "layers.0.mixer.A_b_log 62785.30078125\n",
      "layers.0.mixer.D_b 2048.0\n",
      "layers.0.mixer.in_proj.weight 21.868789672851562\n",
      "layers.0.mixer.conv1d.weight -12.396291732788086\n",
      "layers.0.mixer.conv1d.bias 9.923412322998047\n",
      "layers.0.mixer.x_proj.weight -1.0851119756698608\n",
      "layers.0.mixer.dt_proj.weight 24.908506393432617\n",
      "layers.0.mixer.dt_proj.bias -9511.03515625\n",
      "layers.0.mixer.conv1d_b.weight -21.952051162719727\n",
      "layers.0.mixer.conv1d_b.bias 2.3076255321502686\n",
      "layers.0.mixer.x_proj_b.weight -14.01053237915039\n",
      "layers.0.mixer.dt_proj_b.weight -22.66807746887207\n",
      "layers.0.mixer.dt_proj_b.bias -1.1077779531478882\n",
      "layers.0.mixer.out_proj.weight 0.0\n"
     ]
    }
   ],
   "source": [
    "model = MambaCompressor(d_model=hidden_size, n_layer=1).to(\"cuda\")\n",
    "torch.nn.init.constant_(model.layers[-1].mixer.out_proj.weight, 0)\n",
    "for n, p in model.named_parameters():\n",
    "    if hasattr(p, \"ds_numel\"):\n",
    "        print(n, torch.sum(p.ds_tensor).item())\n",
    "    else:\n",
    "        print(n, torch.sum(p).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, layers.0.mixer.out_proj.weight is initialized from 0.0, which is expected. Also, check the other weights so that they are not nan. You may need to explicitly initialize Mamba module weights if you want to insert this module inside any other model (e.g., LLaVA). For example, check lines 1755-1758 of BIMBA-LLaVA-NeXT/llava/train/train.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we initialize query tokens with the same shape as the output using average pooling. The average pooling gives a good initialization for the output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 24, 24, 1024]) torch.Size([1, 16, 144, 1024])\n"
     ]
    }
   ],
   "source": [
    "pooling = nn.AdaptiveAvgPool3d(target_shape)\n",
    "temporal_pooling = False  # We found not using temporal pooling is good for query initialization\n",
    "\n",
    "# Assume, space_time_tokens represents our input video.\n",
    "space_time_tokens = torch.randn(batch_size, input_shape[0], input_shape[1], input_shape[2], hidden_size).to(\"cuda\")\n",
    "if not temporal_pooling:\n",
    "    query_tokens = space_time_tokens[:,::4]\n",
    "# [1, 16, 24, 24, 1024]\n",
    "query_tokens = query_tokens.permute(0, 4, 1, 2, 3)\n",
    "# [1, 1024, 16, 24, 24]\n",
    "query_tokens = pooling(query_tokens)\n",
    "# [1, 1024, 16, 12, 12]\n",
    "query_tokens = query_tokens.permute(0, 2, 3, 4, 1)\n",
    "# [1, 16, 12, 12, 1024]\n",
    "query_tokens = query_tokens.reshape(batch_size, target_shape[0], -1, hidden_size)\n",
    "print(space_time_tokens.shape, query_tokens.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selective-Scan Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply the MambaCompressor model, which captures fine-grained details from the space_time_tokens into the query_tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 12, 12, 1024])\n"
     ]
    }
   ],
   "source": [
    "query_tokens = model(space_time_tokens, query_tokens)\n",
    "query_tokens = query_tokens.reshape(batch_size,target_shape[0], target_shape[1], target_shape[2], hidden_size)\n",
    "print(query_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query_tokens represents a compressed representation of the space_time_tokens (16x compression ratio), which we can pass to our subsequent model (e.g., LLM) for further efficient processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bimba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
